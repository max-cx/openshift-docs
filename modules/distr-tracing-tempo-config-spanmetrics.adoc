// Module included in the following assemblies:
//
// * distr_tracing_tempo/distr-tracing-tempo-configuring.adoc

:_mod-docs-content-type: REFERENCE
[id="distr-tracing-tempo-config-spanmetrics_{context}"]
= Configuration of the monitor tab in Jaeger UI

Trace data contains rich information, and the data is normalized across instrumented languages and frameworks.
Therefore, additional metrics can be extracted from traces. These metrics are request count, duration, and error count (RED).
The metrics can be visualized in Jaeger console in the *Monitor* tab.

The metrics are derived from spans in the OpenTelemetry Collector that are scraped from the Collector by the Prometheus deployed in the user-workload monitoring stack.
The Jaeger UI queries these metrics from the Prometheus endpoint and visualizes them.

== OpenTelemetry Collector configuration

The OpenTelemetry Collector requires configuration of the `spanmetrics` connector that derives metrics from traces and exports the metrics in the Prometheus format.

.OpenTelemetry Collector custom resource for span RED
[source,yaml]
----
kind: OpenTelemetryCollector
apiVersion: opentelemetry.io/v1alpha1
metadata:
  name: otel
spec:
  mode: deployment
  observability:
    metrics:
      enableMetrics: true # <1>
  config: |
    connectors:
      spanmetrics: # <2>
        metrics_flush_interval: 15s

    receivers:
      otlp: # <3>
        protocols:
          grpc:
          http:

    exporters:
      prometheus: # <4>
        endpoint: 0.0.0.0:8889
        add_metric_suffixes: false
        resource_to_telemetry_conversion:
          enabled: true # by default resource attributes are dropped

      otlp:
        endpoint: "tempo-simplest-distributor:4317"
        tls:
          insecure: true

    service:
      pipelines:
        traces:
          receivers: [otlp]
          exporters: [otlp, spanmetrics] # <5>
        metrics:
          receivers: [spanmetrics] # <6>
          exporters: [prometheus]
----
<1> Creates the `ServiceMonitor` custom resource to enable scraping of the Prometheus exporter.
<2> The Spanmetrics connector receives traces and exports metrics.
<3> The OTLP receiver to receive spans in the OpenTelemetry protocol.
<4> The Prometheus exporter is used to export metrics in the Prometheus format.
<5> The Spanmetrics connector is configured as exporter in traces pipeline.
<6> The Spanmetrics connector is configured as receiver in metrics pipeline.

== Tempo configuration

The `TempoStack` custom resource must specify the following: the *Monitor* tab is enabled, and the Prometheus endpoint is set to the Thanos querier service to query the data from the user-defined monitoring stack.

.TempoStack custom resource with the enabled Monitor tab
[source,yaml]
----
kind:  TempoStack
apiVersion: tempo.grafana.com/v1alpha1
metadata:
  name: simplest
spec:
  template:
    queryFrontend:
      jaegerQuery:
        enabled: true
        monitorTab:
          enabled: true # <1>
          prometheusEndpoint: https://thanos-querier.openshift-monitoring.svc.cluster.local:9091 # <2>
        ingress:
          type: route
----
<1> Enables the monitoring tab in the Jaeger console.
<2> The service name for Thanos Querier from user-workload monitoring.

== Enable alerting on span RED metrics

The metrics generated by the `spanmetrics` connector can be used in alerting rules. For instance to alert on a slow service or define service level objectives (SLOs).
The connector creates `duration_bucket` histogram and `calls` counter metric. These metrics have labels that identify service, API name, operation type and other attributes.

.Labels present on the metrics created oin the `spanmetrics` connector.
[options="header"]
[cols="l, a, a"]
|===
|Label |Description |Values
|service_name
| Service name set by `otel_service_name` environment variable.
|`frontend`

|span_name
| Name of the operation.
|`/`, `/customer`

|span_kind
| Span kind identifies the server, client, messaging or internal operation.
|`SPAN_KIND_SERVER`, `SPAN_KIND_CLIENT`, `SPAN_KIND_PRODUCER`, `SPAN_KIND_CONSUMER`, `SPAN_KIND_INTERNAL`
|===

.PrometheusRule custom resource to define an alert for SLO to serve 95% of requests within 2000ms on the frontend service.
[source,yaml]
----
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: span-red
spec:
  groups:
  - name: server-side-latency
    rules:
    - alert: SpanREDFrontendAPIRequestLatency
      expr: histogram_quantile(0.95, sum(rate(duration_bucket{service_name="frontend", span_kind="SPAN_KIND_SERVER"}[5m])) by (le, service_name, span_name)) > 2000 <1>
      labels:
        severity: Warning
      annotations:
        summary: "High request latency on {{$labels.service_name}} and {{$labels.span_name}}"
        description: "{{$labels.instance}} has 95th request latency above 2s (current value: {{$value}}s)"
----
<1> The expression to check if 95% of frontend server response time is below 2000 ms. The time range (`[5m]`) should be at least four times the scrape interval and long enough to accommodate change in the metric.
